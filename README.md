# Udacity-ComputerVision-Image_Captioning

# Project Overview

![Image Captioning](https://video.udacity-data.com/topher/2018/March/5ab588e3_image-captioning/image-captioning.png)

In this project, you will create a neural network architecture to automatically generate captions from images.

After using the Microsoft Common Objects in Context (MS COCO) dataset to train your network, you will test your network on novel images!

## Project Instructions

The project is structured as a series of Jupyter notebooks that are designed to be completed in sequential order:

- `0_Dataset.ipynb`
- `1_Preliminaries.ipynb`
- `2_Training.ipynb`
- `3_Inference.ipynb`

You can find these notebooks in the Udacity workspace that appears in the concept titled **Project: Image Captioning**. This workspace provides a Jupyter notebook server directly in your browser.

You can read more about workspaces (and how to toggle GPU support) in the following concept (**Introduction to GPU Workspaces**). This concept will show you how to toggle GPU support in the workspace.

You **MUST** enable GPU mode for this project and submit your project after you complete the code in the workspace.

A completely trained model is expected to take between 5-12 hours to train well on a GPU; it is suggested that you look at early patterns in loss (what happens in the first hour or so of training) as you make changes to your model, so that you only have to spend this large amount of time training your final model.

Should you have any questions as you go, please post in the **Student Hub**!

## Evaluation

subset_size need to be increase to improve the captioning, current model is bare minimum to pass
